{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1926230,"sourceType":"datasetVersion","datasetId":1148896}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-17T18:56:52.109216Z","iopub.execute_input":"2024-10-17T18:56:52.109689Z","iopub.status.idle":"2024-10-17T18:56:52.118250Z","shell.execute_reply.started":"2024-10-17T18:56:52.109645Z","shell.execute_reply":"2024-10-17T18:56:52.117309Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"/kaggle/input/en-fr-translation-dataset/en-fr.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport torch\nimport spacy\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import Dataset,DataLoader\nimport time","metadata":{"execution":{"iopub.status.busy":"2024-10-17T18:56:52.127853Z","iopub.execute_input":"2024-10-17T18:56:52.128166Z","iopub.status.idle":"2024-10-17T18:56:52.133791Z","shell.execute_reply.started":"2024-10-17T18:56:52.128133Z","shell.execute_reply":"2024-10-17T18:56:52.132784Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"spacy_eng = spacy.load(\"en_core_web_sm\")\n# hyper parameters\ndevice=('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2024-10-17T18:56:52.147772Z","iopub.execute_input":"2024-10-17T18:56:52.148064Z","iopub.status.idle":"2024-10-17T18:56:53.633158Z","shell.execute_reply.started":"2024-10-17T18:56:52.148032Z","shell.execute_reply":"2024-10-17T18:56:53.632374Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"class vocabulary():\n    def __init__(self,freq_threshold):\n        self.itos={0:'PAD',1:'<SOS>',2:'<EOS>',3:'<UNK>'}\n        self.stois={'PAD':0,'<SOS>':1,'<EOS>':2,'<UNK>':3}\n        self.freq_threshold=freq_threshold\n        \n    def __len__(self):\n        return len(self.stois)\n    \n    @staticmethod\n    def tokenised_text(text):\n        return [token.text.lower() for token in spacy_eng.tokenizer(str(text))]\n    \n    def build_vocabulary(self,sentence_list):\n        frequencies={}\n        idx=4\n        \n        for sentence in sentence_list:\n            for word in self.tokenised_text(sentence):\n                if word not in frequencies:\n                    frequencies[word]=1\n                    \n                else:\n                    frequencies[word]+=1\n                    \n                if frequencies[word]==self.freq_threshold:\n                    self.stois[word]=idx\n                    self.itos[idx]=word\n                    idx+=1\n                    \n    def numericalise(self,text):\n        token_text=self.tokenised_text(text)\n        \n        return [self.stois[word] if word in self.stois else self.stois['<UNK>'] for word in token_text]\n                    \n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T18:56:53.635147Z","iopub.execute_input":"2024-10-17T18:56:53.635891Z","iopub.status.idle":"2024-10-17T18:56:53.645637Z","shell.execute_reply.started":"2024-10-17T18:56:53.635836Z","shell.execute_reply":"2024-10-17T18:56:53.644581Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"\nclass custom_data_loader(Dataset):\n    def __init__(self,file,freq_threshold):\n        super().__init__()\n        \n        self.df=pd.read_csv(file,nrows=100000)\n        self.freq_threshold=freq_threshold\n#         get target and source sequences\n        self.source_seq=self.df.iloc[:,0]\n        self.target_seq=self.df.iloc[:,1]\n        \n        self.vocab_source=vocabulary(freq_threshold=self.freq_threshold)\n        self.vocab_target=vocabulary(freq_threshold=self.freq_threshold)\n        \n        self.vocab_source.build_vocabulary(self.source_seq.tolist())\n        self.vocab_target.build_vocabulary(self.target_seq.tolist())\n        self.len_vocab_for_model=max(len(self.vocab_source.itos),len(self.vocab_target.itos))\n        \n\n        \n    def __len__(self):\n        return len(self.source_seq)\n    \n    def __getitem__(self,index):\n        source=self.source_seq[index]\n        target=self.target_seq[index]\n        \n        \n        numericalized_source = [self.vocab_source.stois[\"<SOS>\"]]\n        numericalized_source += self.vocab_source.numericalise(source)\n        numericalized_source.append(self.vocab_source.stois[\"<EOS>\"])\n        \n        numericalized_caption = [self.vocab_target.stois[\"<SOS>\"]]\n        numericalized_caption += self.vocab_target.numericalise(target)\n        numericalized_caption.append(self.vocab_target.stois[\"<EOS>\"])\n        \n        return numericalized_source,numericalized_caption\n\n        ","metadata":{"execution":{"iopub.status.busy":"2024-10-17T18:56:53.646945Z","iopub.execute_input":"2024-10-17T18:56:53.647239Z","iopub.status.idle":"2024-10-17T18:56:53.660049Z","shell.execute_reply.started":"2024-10-17T18:56:53.647208Z","shell.execute_reply":"2024-10-17T18:56:53.659063Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"        \nclass MyCollate:\n    def __init__(self,pad_idx,fixed_length):\n        self.pad_idx=pad_idx\n        self.fixed_length=fixed_length\n        \n    def __call__(self,batch):\n        inputs=[source[0]  for source in batch]\n        targets=[source[1]  for source in batch]\n        \n        # Pad inputs and targets using TensorFlow pad_sequences\n        inputs = pad_sequences(inputs, maxlen=self.fixed_length, padding='post', truncating='post', value=self.pad_idx)\n        targets = pad_sequences(targets, maxlen=self.fixed_length, padding='post', truncating='post', value=self.pad_idx)\n        \n        # Convert to PyTorch tensors\n        inputs = torch.tensor(inputs, dtype=torch.long)\n        targets = torch.tensor(targets, dtype=torch.long)\n        return inputs, targets\n    \ndef get_loader(\n    \n    file,\n    freq_threshold,\n    batch_size=16,\n#     num_workers=1,\n    shuffle=True,\n    pin_memory=True,\n    \n):\n    \n    dataset=custom_data_loader(file,freq_threshold)\n    \n    pad_idx = dataset.vocab_source.stois[\"PAD\"]\n    fixed_length=32\n    loader = DataLoader(\n        dataset=dataset,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        shuffle=shuffle,\n        pin_memory=pin_memory,\n        collate_fn=MyCollate(pad_idx=pad_idx,fixed_length=fixed_length),\n    )\n    return loader,dataset.len_vocab_for_model","metadata":{"execution":{"iopub.status.busy":"2024-10-17T18:56:53.662149Z","iopub.execute_input":"2024-10-17T18:56:53.662462Z","iopub.status.idle":"2024-10-17T18:56:53.676484Z","shell.execute_reply.started":"2024-10-17T18:56:53.662430Z","shell.execute_reply":"2024-10-17T18:56:53.675522Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# parameters\nfreq_threshold=25\nfile=\"/kaggle/input/en-fr-translation-dataset/en-fr.csv\"\n\nloader,len_vocab_for_model = get_loader(file=file,freq_threshold=freq_threshold)\n\n    ","metadata":{"execution":{"iopub.status.busy":"2024-10-17T18:56:53.677445Z","iopub.execute_input":"2024-10-17T18:56:53.677731Z","iopub.status.idle":"2024-10-17T18:57:22.868337Z","shell.execute_reply.started":"2024-10-17T18:56:53.677701Z","shell.execute_reply":"2024-10-17T18:57:22.867538Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nclass encoder(nn.Module):\n    def __init__(self,length_of_vocab):\n        super().__init__()\n        self.length_of_vocab=length_of_vocab\n        self.embedding_layer=nn.Embedding(self.length_of_vocab,3,padding_idx=0)\n        self.lstm_layer1=nn.LSTM(3,4,1)\n        \n    def forward(self,x):\n        x=self.embedding_layer(x)\n        output,hidden_states =self.lstm_layer1(x)\n        return output,hidden_states\n    \n\n\nclass decoder(nn.Module):\n    def __init__(self,length_of_vocab):\n        super().__init__()\n        self.length_of_vocab=length_of_vocab\n        self.encoder_obj=encoder(length_of_vocab)  \n        self.embedding_layer=nn.Embedding(self.length_of_vocab,3,padding_idx=0)\n        self.lstm_layer=nn.LSTM(3,4,1)\n        self.linear=nn.Linear(4,self.length_of_vocab)\n        \n    def forward(self,source_seq,target_seq):\n        \n        output,hidden_states=self.encoder_obj(source_seq)\n        embeddings=self.embedding_layer(target_seq)\n     \n        output_dec,_=self.lstm_layer(embeddings,hidden_states)\n        final=self.linear(output_dec)\n        return final\n    \n\n        \n    \n    ","metadata":{"execution":{"iopub.status.busy":"2024-10-17T18:57:22.869646Z","iopub.execute_input":"2024-10-17T18:57:22.870297Z","iopub.status.idle":"2024-10-17T18:57:22.879728Z","shell.execute_reply.started":"2024-10-17T18:57:22.870243Z","shell.execute_reply":"2024-10-17T18:57:22.878818Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"model=decoder(len_vocab_for_model).to(device)\nlr=0.05\nepochs=5\noptimiser=optim.Adam(model.parameters(),lr=lr)\ncriteria=nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2024-10-17T18:57:22.880791Z","iopub.execute_input":"2024-10-17T18:57:22.881067Z","iopub.status.idle":"2024-10-17T18:57:22.901156Z","shell.execute_reply.started":"2024-10-17T18:57:22.881037Z","shell.execute_reply":"2024-10-17T18:57:22.900206Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"losses_list=[]\nmodel.train()\nfor epoch in range(epochs):\n    start = time.time()\n    for source_seq,target_seq in loader:\n               \n        source_seq=source_seq.to(device)\n        target_seq=target_seq.to(device)\n        \n        output=model.forward(source_seq,target_seq)\n        \n        # Reshape logits to (batch_size * sequence_length, num_classes)\n        logits = output.view(-1, len_vocab_for_model)  # shape: (30, 5)\n\n        # Reshape labels to (batch_size * sequence_length)\n        labels = target_seq.view(-1)  # shape: (30,)\n\n        loss=criteria(logits,labels)\n        losses_list.append(loss.item())\n\n        optimiser.zero_grad()    \n        loss.backward()\n        optimiser.step()\n    \n    \n    mean_loss = sum(losses_list) / len(losses_list)\n    end = time.time()\n    print(f'Loss after {epoch} is {mean_loss} and it took {end-start} seconds')    ","metadata":{"execution":{"iopub.status.busy":"2024-10-17T18:57:22.902117Z","iopub.execute_input":"2024-10-17T18:57:22.902441Z","iopub.status.idle":"2024-10-17T19:00:38.318231Z","shell.execute_reply.started":"2024-10-17T18:57:22.902408Z","shell.execute_reply":"2024-10-17T19:00:38.317098Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"name":"stdout","text":"Loss after 0 is 1.9860015284347534 and it took 39.27575397491455 seconds\nLoss after 1 is 1.6469195326042176 and it took 39.05873894691467 seconds\nLoss after 2 is 1.4575135706710816 and it took 39.03929543495178 seconds\nLoss after 3 is 1.3296277607572078 and it took 38.998775482177734 seconds\nLoss after 4 is 1.2363820798521041 and it took 39.031928062438965 seconds\n","output_type":"stream"}]}]}